{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodolfocacacho/miniforge3/envs/lsd/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rodolfocacacho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rodolfocacacho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m file_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/subtask1/validation.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Open the JSON file\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mfile\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Load the JSON data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create a list of dictionaries\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "file_train = 'data/subtask1/train.json'\n",
    "file_validation = 'data/subtask1/validation.json'\n",
    "\n",
    "# Open the JSON file\n",
    "with open(file, 'r') as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a list of dictionaries\n",
    "entries_list = []\n",
    "\n",
    "# Iterate through each entry in the JSON data\n",
    "for entry in data:\n",
    "    # Extract relevant information from each entry\n",
    "    entry_dict = {\n",
    "        \"id\": entry[\"id\"],\n",
    "        \"text\": entry[\"text\"],\n",
    "        \"labels\": entry[\"labels\"],\n",
    "        \"link\": entry[\"link\"]\n",
    "    }\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    entries_list.append(entry_dict)\n",
    "\n",
    "# Now 'entries_list' contains a list of dictionaries, each representing an entry in your JSON file\n",
    "# You can access the information as needed\n",
    "for entry_dict in entries_list:\n",
    "    print(f\"ID: {entry_dict['id']}, Text: {entry_dict['text']}, Labels: {entry_dict['labels']}, Link: {entry_dict['link']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list to store the preprocessed dictionaries\n",
    "preprocessed_list = []\n",
    "\n",
    "for entry in entries_list:\n",
    "    preprocessed_entry = entry.copy()\n",
    "    preprocessed_entry['text'] = preprocess_text(entry['text'])\n",
    "    if len(preprocessed_entry['labels']) == 0:\n",
    "        preprocessed_entry['labels'] = ['nocat']\n",
    "    else:\n",
    "        t = preprocessed_entry.get('labels',[0])[0]\n",
    "        # print(f'aos {t}')\n",
    "        preprocessed_entry['labels'] = [t]\n",
    "        # preprocessed_entry['labels'] = preprocessed_entry['labels']    \n",
    "    preprocessed_list.append(preprocessed_entry)\n",
    "\n",
    "\n",
    "# Display the preprocessed data\n",
    "for id,entry in enumerate(preprocessed_list):\n",
    "    print(f\"ID: {entry['id']}, Text: {entry['text']} Label: {entry['labels']}\")\n",
    "    # print(f\"ID: {entries_list[id]['id']}, Text: {entries_list[id]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m max_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Loop through the dataset\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpreprocessed_list\u001b[49m:\n\u001b[0;32m      9\u001b[0m     labels \u001b[38;5;241m=\u001b[39m entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m, [])  \u001b[38;5;66;03m# Get the labels for the current entry\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m labels:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to store label counts per category\n",
    "category_counts = {}\n",
    "ids_ohne_label = []\n",
    "ids_mit_label = []\n",
    "max_id = 0\n",
    "\n",
    "# Loop through the dataset\n",
    "for entry in preprocessed_list:\n",
    "    labels = entry.get('labels', [])  # Get the labels for the current entry\n",
    "\n",
    "    if not labels:\n",
    "        ids_ohne_label.append(entry['id'])\n",
    "    else:\n",
    "        ids_mit_label.append(entry['id'])\n",
    "        for label in labels:\n",
    "            if label not in category_counts:\n",
    "                category_counts[label] = 1\n",
    "            else:\n",
    "                category_counts[label] += 1\n",
    "    if len(labels) > max_id:\n",
    "        max_id = len(labels)\n",
    "        idx = entry['id']\n",
    "\n",
    "\n",
    "# Display the label counts per category\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"Category: {category}, Count: {count}\")\n",
    "\n",
    "f_list = [d for d in preprocessed_list if d.get('id') == idx]\n",
    "\n",
    "print(f'ids ohne labels: {len(ids_ohne_label)} ids mit labels: {len(ids_mit_label)} max_labels {max_id}')\n",
    "print(f'id {idx} text: {f_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b852811e1240490ba08923e49c8bf29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\python3.9\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\meikf\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d1b53037da45a28e7167f4ef5e6748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395f5677e4fe43aba3431e38d1ab5a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088f824d4b494f6cb52dad99934521d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenize the text for each entry in the dataset\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpreprocessed_list\u001b[49m:\n\u001b[0;32m      6\u001b[0m     text \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the text for each entry in the dataset\n",
    "for entry in preprocessed_list:\n",
    "    text = entry['text']\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Update the entry with tokenized information\n",
    "    entry['input_ids'] = tokens['input_ids']\n",
    "    entry['attention_mask'] = tokens['attention_mask']\n",
    "\n",
    "# Display the tokenized entries\n",
    "for entry in preprocessed_list:\n",
    "    print(f\"ID: {entry['id']}\")\n",
    "    print(f\"Text: {entry['text']}\")\n",
    "    print(\"Token IDs:\", entry['input_ids'])\n",
    "    print(\"Attention Mask:\", entry['attention_mask'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace multiple whitespace characters with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Replace line breaks with a space\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Replace dashes with spaces\n",
    "    text = text.replace('-', ' ')\n",
    "\n",
    "    # Remove special characters, punctuation (except apostrophes), and symbols\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Porter stemming (optional)\n",
    "    # stemmer = PorterStemmer()\n",
    "    # tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def read_json_labels(file):\n",
    "    # Open the JSON file\n",
    "    with open(file, 'r') as file:\n",
    "        # Load the JSON data\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Create a list of dictionaries\n",
    "    entries_list = []\n",
    "    # Iterate through each entry in the JSON data\n",
    "    for entry in data:\n",
    "        # Extract relevant information from each entry\n",
    "        entry_dict = {\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"text\": entry[\"text\"],\n",
    "            \"labels\": entry[\"labels\"],\n",
    "            \"link\": entry[\"link\"]\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        entries_list.append(entry_dict)\n",
    "    \n",
    "    return entries_list\n",
    "\n",
    "\n",
    "def preprocess_text_list(entries_list):\n",
    "    # Create a new list to store the preprocessed dictionaries\n",
    "    preprocessed_list = []\n",
    "\n",
    "    for entry in entries_list:\n",
    "        preprocessed_entry = entry.copy()\n",
    "        preprocessed_entry['text'] = preprocess_text(entry['text'])\n",
    "        if len(preprocessed_entry['labels']) == 0:\n",
    "            preprocessed_entry['labels'] = ['nocat']\n",
    "        else:\n",
    "            t = preprocessed_entry.get('labels',[0])[0]\n",
    "            # print(f'aos {t}')\n",
    "            preprocessed_entry['labels'] = [t]\n",
    "            # preprocessed_entry['labels'] = preprocessed_entry['labels']    \n",
    "        preprocessed_list.append(preprocessed_entry)\n",
    "    \n",
    "    return preprocessed_list\n",
    "\n",
    "def preprocess_labels(entries_list):\n",
    "\n",
    "    # Create a new list to store the preprocessed dictionaries\n",
    "    preprocessed_list = []\n",
    "\n",
    "    for entry in entries_list:\n",
    "        preprocessed_entry = entry.copy()\n",
    "        preprocessed_entry['text'] = preprocess_text(entry['text'])\n",
    "        if len(preprocessed_entry['labels']) == 0:\n",
    "            preprocessed_entry['labels'] = ['nocat']\n",
    "        else:\n",
    "            t = preprocessed_entry.get('labels',[0])[0]\n",
    "            # print(f'aos {t}')\n",
    "            preprocessed_entry['labels'] = [t]\n",
    "            # preprocessed_entry['labels'] = preprocessed_entry['labels']    \n",
    "        preprocessed_list.append(preprocessed_entry)\n",
    "\n",
    "    return preprocessed_list\n",
    "\n",
    "# def tokenize_text(preprocessed_list):\n",
    "#     # Tokenize the text for each entry in the dataset\n",
    "#     for entry in preprocessed_list:\n",
    "#         text = entry['text']\n",
    "#         tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "#         # Update the entry with tokenized information\n",
    "#         entry['input_ids'] = tokens['input_ids']\n",
    "#         entry['attention_mask'] = tokens['attention_mask']\n",
    "    \n",
    "#     return preprocessed_list\n",
    "\n",
    "def tokenize_text(preprocessed_list, tokenizer):\n",
    "    tokenized_list = []  # Create a new list to store tokenized entries\n",
    "\n",
    "    # Tokenize the text for each entry in the dataset\n",
    "    for entry in preprocessed_list:\n",
    "        text = entry['text']\n",
    "        tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Create a new entry with tokenized information\n",
    "        tokenized_entry = {\n",
    "            'id': entry['id'],\n",
    "            'input_ids': tokens['input_ids'],\n",
    "            'attention_mask': tokens['attention_mask']\n",
    "        }\n",
    "\n",
    "        tokenized_list.append(tokenized_entry)\n",
    "\n",
    "    return tokenized_list\n",
    "\n",
    "def extract_lists(data_list):\n",
    "    texts = []\n",
    "    ids = []\n",
    "    labels = []\n",
    "\n",
    "    for entry in data_list:\n",
    "        text = entry.get('text', '')\n",
    "        id_value = entry.get('id', '')\n",
    "        label_value = entry.get('labels', '')\n",
    "\n",
    "        texts.append(text)\n",
    "        ids.append(id_value)\n",
    "        labels.extend(label_value)\n",
    "        # labels.append(label_value)\n",
    "\n",
    "    return texts, ids, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = 'data/subtask1/train.json'\n",
    "file_validation = 'data/subtask1/validation.json'\n",
    "# Create or load a label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Train\n",
    "train_json = read_json_labels(file_train)\n",
    "train_list = preprocess_text_list(train_json)\n",
    "train_list = preprocess_labels(train_list)\n",
    "train_text_list, train_id_list, train_labels_list = extract_lists(train_list)\n",
    "train_encoded_labels = label_encoder.fit_transform(train_labels_list)\n",
    "train_tokens = tokenizer(train_text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "train_tokens_input_ids = train_tokens['input_ids']\n",
    "train_tokens_mask_ids = train_tokens['attention_mask']\n",
    "\n",
    "# train_list = tokenize_text(train_list,tokenizer)\n",
    "\n",
    "# # Validation\n",
    "val_json = read_json_labels(file_validation)\n",
    "val_list = preprocess_text_list(val_json)\n",
    "val_list = preprocess_labels(val_list)\n",
    "val_text_list, val_id_list, val_labels_list = extract_lists(val_list)\n",
    "val_encoded_labels = label_encoder.transform(val_labels_list)\n",
    "val_tokens = tokenizer(val_text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_tokens_input_ids = val_tokens['input_ids']\n",
    "val_tokens_mask_ids = val_tokens['attention_mask']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/rodolfocacacho/miniforge3/envs/lsd/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_tokens_input_ids, train_tokens_mask_ids, torch.tensor(train_encoded_labels,dtype=torch.long))\n",
    "val_dataset = TensorDataset(val_tokens_input_ids, val_tokens_mask_ids, torch.tensor(val_encoded_labels,dtype=torch.long))\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "model_name = 'roberta-base'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader))\n",
    "\n",
    "# Loss Function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Training Loss: 3.0380727450052896\n",
      "preds tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.]) labels tensor([ 5.,  6., 18.,  8.,  0.,  9., 20., 17.,  7.,  0.,  0.,  3., 17.,  9.,\n",
      "         0., 11., 17., 11.,  5.,  5.])\n",
      "f1 0.0 size 1\n",
      "Epoch 1/3, Validation F1-Score: 0.0\n",
      "Epoch 2/3, Average Training Loss: 3.0482051372528076\n",
      "preds tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.]) labels tensor([ 5.,  6., 18.,  8.,  0.,  9., 20., 17.,  7.,  0.,  0.,  3., 17.,  9.,\n",
      "         0., 11., 17., 11.,  5.,  5.])\n",
      "f1 0.0 size 1\n",
      "Epoch 2/3, Validation F1-Score: 0.0\n",
      "Epoch 3/3, Average Training Loss: 3.033432881037394\n",
      "preds tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.]) labels tensor([ 5.,  6., 18.,  8.,  0.,  9., 20., 17.,  7.,  0.,  0.,  3., 17.,  9.,\n",
      "         0., 11., 17., 11.,  5.,  5.])\n",
      "f1 0.0 size 1\n",
      "Epoch 3/3, Validation F1-Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch tensors to the same device as the model\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Print average training loss for the epoch\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Training Loss: {average_loss}\")\n",
    "\n",
    " # Validation\n",
    "    model.eval()\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            # print(f'logits {logits} size {logits.size()}')\n",
    "            # val_preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "            val_preds = torch.argmax(logits, dim=1)\n",
    "            # print(f'val_preds {val_preds} size {val_preds.size()}')\n",
    "            \n",
    "            all_val_labels.append(labels.numpy())\n",
    "            all_val_preds.append(val_preds.numpy())\n",
    "\n",
    "\n",
    "    # Flatten the list of numpy arrays\n",
    "    all_val_labels = np.concatenate(all_val_labels)\n",
    "    all_val_preds = np.concatenate(all_val_preds)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    all_val_labels = torch.tensor(all_val_labels).float()\n",
    "    all_val_preds = torch.tensor(all_val_preds).float()\n",
    "\n",
    "    print(f'preds {all_val_preds} labels {all_val_labels}')\n",
    "    # Calculate F1-score for validation set\n",
    "    f1 = f1_score(all_val_labels, all_val_preds, average='micro',zero_division=\"warn\")\n",
    "    print(f'f1 {f1} size {f1.size}')\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
