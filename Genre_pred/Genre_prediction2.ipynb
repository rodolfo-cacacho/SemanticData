{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('original_data.txt',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               drama \n",
       "1            thriller \n",
       "2               adult \n",
       "3               drama \n",
       "4               drama \n",
       "             ...      \n",
       "54209          comedy \n",
       "54210          horror \n",
       "54211     documentary \n",
       "54212          comedy \n",
       "54213         history \n",
       "Name: genre, Length: 54214, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            genre  Count\n",
      "0         action    1315\n",
      "1          adult     590\n",
      "2      adventure     775\n",
      "3      animation     498\n",
      "4      biography     265\n",
      "5         comedy    7447\n",
      "6          crime     505\n",
      "7    documentary   13096\n",
      "8          drama   13613\n",
      "9         family     784\n",
      "10       fantasy     323\n",
      "11     game-show     194\n",
      "12       history     243\n",
      "13        horror    2204\n",
      "14         music     731\n",
      "15       musical     277\n",
      "16       mystery     319\n",
      "17          news     181\n",
      "18    reality-tv     884\n",
      "19       romance     672\n",
      "20        sci-fi     647\n",
      "21         short    5073\n",
      "22         sport     432\n",
      "23     talk-show     391\n",
      "24      thriller    1591\n",
      "25           war     132\n",
      "26       western    1032\n"
     ]
    }
   ],
   "source": [
    "category_counts = data.groupby('genre').size().reset_index(name='Count')\n",
    "\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data len: 43371 \n",
      "test_data len: 10843\n"
     ]
    }
   ],
   "source": [
    "# Use LabelEncoder to encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['genre'])\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(f'train_data len: {len(train_data)} \\ntest_data len: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_rows(df):\n",
    "    for index, row in df.iterrows():\n",
    "        item = row['description'],row['label']\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' Sex. Betrayal. Seduction. Manipulation. Addiction. Love. Hate. Vanity. Obsession. Words that brand each of the 8 characters of SHADE. Linda, Ty, Aurora, Delilah, Cassius, D-Low, Sega and John. Each a master at presenting carefully woven personas and each orchestrating solitary gambits of depravity. SHADE transcends convention, decorum and time, plunging the viewer down to the abyss in which we all reside but few embrace. Between the darkness and the light exists the shade. In that shade lives our characters.',\n",
       " 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterate_rows(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = iterate_rows(train_data)\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[593, 349, 9, 13213]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['my', 'name', 'is', 'mohammed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _text, _label  in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "\n",
    "    return text_list.to(device),label_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "\n",
    "train_iter = iterate_rows(train_data)\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model, loss function, and optimizer\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False,include_last_offset=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "    \n",
    "train_iter = iterate_rows(train_data)\n",
    "num_class = len(label_encoder.classes_)\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "\n",
    "# model = TextClassificationModel(vocab_size, emsize, num_class)\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text,label, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/  644 batches | accuracy    0.409\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  7.57s | valid accuracy    0.431 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/  644 batches | accuracy    0.459\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  7.68s | valid accuracy    0.464 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/  644 batches | accuracy    0.491\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  7.82s | valid accuracy    0.491 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/  644 batches | accuracy    0.519\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  7.95s | valid accuracy    0.499 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/  644 batches | accuracy    0.540\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  7.92s | valid accuracy    0.512 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/  644 batches | accuracy    0.555\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  7.75s | valid accuracy    0.520 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/  644 batches | accuracy    0.569\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  7.78s | valid accuracy    0.527 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/  644 batches | accuracy    0.585\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  7.86s | valid accuracy    0.541 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/  644 batches | accuracy    0.597\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  7.99s | valid accuracy    0.528 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/  644 batches | accuracy    0.616\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  7.82s | valid accuracy    0.552 \n",
      "-----------------------------------------------------------\n",
      "| epoch  11 |   500/  644 batches | accuracy    0.621\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time:  7.84s | valid accuracy    0.554 \n",
      "-----------------------------------------------------------\n",
      "| epoch  12 |   500/  644 batches | accuracy    0.619\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  12 | time:  8.14s | valid accuracy    0.555 \n",
      "-----------------------------------------------------------\n",
      "| epoch  13 |   500/  644 batches | accuracy    0.623\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  13 | time:  9.16s | valid accuracy    0.555 \n",
      "-----------------------------------------------------------\n",
      "| epoch  14 |   500/  644 batches | accuracy    0.623\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  14 | time:  8.16s | valid accuracy    0.554 \n",
      "-----------------------------------------------------------\n",
      "| epoch  15 |   500/  644 batches | accuracy    0.627\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  15 | time:  8.20s | valid accuracy    0.555 \n",
      "-----------------------------------------------------------\n",
      "| epoch  16 |   500/  644 batches | accuracy    0.626\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  16 | time:  8.13s | valid accuracy    0.556 \n",
      "-----------------------------------------------------------\n",
      "| epoch  17 |   500/  644 batches | accuracy    0.627\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  17 | time:  8.13s | valid accuracy    0.556 \n",
      "-----------------------------------------------------------\n",
      "| epoch  18 |   500/  644 batches | accuracy    0.628\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  18 | time:  8.18s | valid accuracy    0.557 \n",
      "-----------------------------------------------------------\n",
      "| epoch  19 |   500/  644 batches | accuracy    0.624\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  19 | time:  9.13s | valid accuracy    0.555 \n",
      "-----------------------------------------------------------\n",
      "| epoch  20 |   500/  644 batches | accuracy    0.629\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  20 | time:  9.91s | valid accuracy    0.555 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 20  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = iterate_rows(train_data), iterate_rows(test_data)\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.562\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a  comedy  movie\n"
     ]
    }
   ],
   "source": [
    "class_dict = dict(zip(range(len(label_encoder.classes_)),label_encoder.classes_))\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() \n",
    "\n",
    "\n",
    "ex_text_str = \"Three buddies wake up from a bachelor party in Las Vegas, with no memory of the previous night and the bachelor missing. They make their way around the city in order to find their friend before his wedding.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s movie\" % class_dict[predict(ex_text_str, text_pipeline)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
